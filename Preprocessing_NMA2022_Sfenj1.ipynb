{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing_NMA2022_Sfenj1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {	
      "cell_type": "markdown",
      "source": [
        "# Links\n",
        "\n",
        "[Open main branch version of this notebook in Colab](https://colab.research.google.com/github/ffvoigt/neuromatch-compneuro-2022-project/blob/main/Preprocessing_NMA2022_Sfenj1.ipynb)\n",
        "\n",
        "[Open development branch version of this notebook in Colab](https://colab.research.google.com/github/ffvoigt/neuromatch-compneuro-2022-project/blob/development/Preprocessing_NMA2022_Sfenj1.ipynb)\n"      
      ],
      "metadata": {
        "id": "LinkCell"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Caltech_preprocessing notebook \n",
        "\n",
        "\n",
        "*   The purpose of this notebook is to download the tracking dataset from the CalTech database and convert it to a .csv file that can be loaded using pandas. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GlU7P5s_JbSY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vb7N-4zgDFXm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download and unzip the data\n",
        "import os, requests, zipfile\n",
        "\n",
        "fname = 'task1.zip'\n",
        "url = \"https://data.caltech.edu/tindfiles/serve/a86f4297-a087-4f40-9ed4-765779105c2c/\"\n",
        "\n",
        "if not os.path.isfile(fname):\n",
        "  try:\n",
        "    r = requests.get(url)\n",
        "  except requests.ConnectionError:\n",
        "    print(\"!!! Failed to download data !!!\")\n",
        "  else:\n",
        "    if r.status_code != requests.codes.ok:\n",
        "      print(\"!!! Failed to download data !!!\")\n",
        "    else:\n",
        "      with open(fname, \"wb\") as fid:\n",
        "        fid.write(r.content)\n",
        "else:\n",
        "  print('Data have already been downloaded!!!')\n",
        "\n",
        "if not os.path.exists('task1_classic_classification'):\n",
        "  # Unzip the file\n",
        "  with zipfile.ZipFile(fname, 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')\n",
        "\n",
        "\n",
        "# Download the script\n",
        "fname = 'calms21_convert_to_npy.py'\n",
        "url = \"https://data.caltech.edu/tindfiles/serve/ca84a583-ea06-440a-995c-c184bcb0291c/\"\n",
        "\n",
        "if not os.path.isfile(fname):\n",
        "  try:\n",
        "    r = requests.get(url)\n",
        "  except requests.ConnectionError:\n",
        "    print(\"!!! Failed to download data !!!\")\n",
        "  else:\n",
        "    if r.status_code != requests.codes.ok:\n",
        "      print(\"!!! Failed to download data !!!\")\n",
        "    else:\n",
        "      with open(fname, \"wb\") as fid:\n",
        "        fid.write(r.content)"
      ],
      "metadata": {
        "id": "7SgraLGJD9jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convert .json files to .npy files "
      ],
      "metadata": {
        "id": "XiKghy6VJRU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python calms21_convert_to_npy.py  --input_directory '.' --output_directory '.'\n",
        "!python calms21_convert_to_npy.py  --input_directory '.' --output_directory '.' --parse_treba"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvdcRzdkD-Zf",
        "outputId": "c49ee340-d719-4008-a3e5-e2546b3b0ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ./calms21_task1_test\n",
            "tcmalloc: large alloc 1224941568 bytes == 0x4e832000 @  0x7f3f6ec451e7 0x4a3940 0x5b438c 0x5ea94f 0x5939cb 0x594cd3 0x5d0ecb 0x5939af 0x594cd3 0x594f8e 0x59526e 0x5bfba0 0x59aeca 0x515655 0x549e0e 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x5118f8 0x593dd7 0x5118f8 0x549576 0x604173 0x5f5506 0x5f8c6c 0x5f9206 0x64faf2 0x64fc4e 0x7f3f6e842c87\n",
            "tcmalloc: large alloc 1224941568 bytes == 0x97864000 @  0x7f3f6ec451e7 0x4a3940 0x52ab72 0x527cf3 0x51d358 0x59358d 0x548c51 0x51566f 0x549576 0x4bcb19 0x59c019 0x59588e 0x595e64 0x4d8924 0x5bfbcb 0x59aeca 0x515655 0x549e0e 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x5118f8 0x593dd7 0x5118f8 0x549576 0x604173 0x5f5506 0x5f8c6c 0x5f9206\n",
            "Saving ./calms21_task1_train\n",
            "Saving ./calms21_task1_test_features\n",
            "tcmalloc: large alloc 1224941568 bytes == 0xbd92000 @  0x7f8019a241e7 0x4a3940 0x5b438c 0x5ea94f 0x5939cb 0x594cd3 0x5d0ecb 0x5939af 0x594cd3 0x594f8e 0x59526e 0x5bfba0 0x59aeca 0x515655 0x549e0e 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x5118f8 0x593dd7 0x5118f8 0x549576 0x604173 0x5f5506 0x5f8c6c 0x5f9206 0x64faf2 0x64fc4e 0x7f8019621c87\n",
            "tcmalloc: large alloc 1224941568 bytes == 0x54dc4000 @  0x7f8019a241e7 0x4a3940 0x52ab72 0x527cf3 0x51d358 0x59358d 0x548c51 0x51566f 0x549576 0x4bcb19 0x59c019 0x59588e 0x595e64 0x4d8924 0x5bfbcb 0x59aeca 0x515655 0x549e0e 0x593fce 0x548ae9 0x51566f 0x549576 0x593fce 0x5118f8 0x593dd7 0x5118f8 0x549576 0x604173 0x5f5506 0x5f8c6c 0x5f9206\n",
            "Saving ./calms21_task1_train_features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load task data from converted .npy files"
      ],
      "metadata": {
        "id": "kk2WR3MsKPPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_task1_data(data_path):\n",
        "  \"\"\"\n",
        "  Load data for task 1:\n",
        "      The vocaubulary tells you how to map behavior names to class ids;\n",
        "      it is the same for all sequences in this dataset.\n",
        "  \"\"\"\n",
        "  data_dict = np.load(data_path, allow_pickle=True).item()\n",
        "  dataset = data_dict['annotator-id_0']\n",
        "  # Get any sequence key.\n",
        "  sequence_id = list(data_dict['annotator-id_0'].keys())[0]\n",
        "  vocabulary = data_dict['annotator-id_0'][sequence_id]['metadata']['vocab']\n",
        "  return dataset, vocabulary\n",
        "\n",
        "training_data, vocab = load_task1_data('./calms21_task1_train.npy')\n",
        "test_data, _ = load_task1_data('./calms21_task1_test.npy')"
      ],
      "metadata": {
        "id": "5NZCvYYDEC9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Parse training_data dictionary across all sessions and animals to return tracking and metadata in a workable dataframe format. "
      ],
      "metadata": {
        "id": "1qhRODu3I9Vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate body part names associated with pose tracking\n",
        "pos_x_names = ['nose_X', 'left_ear_X', 'neck_X', 'right_ear_X', 'left_hip_X', 'right_hip_X', 'tailbase_X'] \n",
        "pos_y_names = ['nose_Y', 'left_ear_Y', 'neck_Y','right_ear_Y', 'left_hip_Y', 'right_hip_Y', 'tailbase_Y']\n",
        "tracking_df = pd.DataFrame({'frame_num':[], 'session_num': [], 'mouse_id': [], 'annotations':[]})\n",
        "training_df = pd.DataFrame(training_data)\n",
        "for idx, session in enumerate(training_df.columns):\n",
        "  for mouse in [0,1]:\n",
        "    session_df = pd.DataFrame()\n",
        "    session_df['annotations']=training_df[session]['annotations']\n",
        "    session_df['session_num'] = idx\n",
        "    session_df['mouse_id'] = mouse\n",
        "    session_df['frame_num'] = range(len(training_df[session]['annotations']))\n",
        "    for x in range(len(pos_x_names)):\n",
        "      session_df[pos_x_names[x]] = training_df[session]['keypoints'][:,mouse,0,x]\n",
        "      session_df[pos_y_names[x]] = training_df[session]['keypoints'][:,mouse,1,x]\n",
        "    tracking_df = tracking_df.append(session_df, ignore_index=True)\n",
        "\n",
        "# change discrete values to int\n",
        "tracking_df.frame_num = tracking_df.frame_num.astype('int64')\n",
        "tracking_df.session_num = tracking_df.session_num.astype('int64')\n",
        "tracking_df.mouse_id = tracking_df.mouse_id.astype('int64')\n",
        "tracking_df.annotations = tracking_df.annotations.astype('int64')"
      ],
      "metadata": {
        "id": "yF-mxUktEw3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export dataframe to .csv"
      ],
      "metadata": {
        "id": "BIr2HZeWLh8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tracking_df.to_csv('tracking_df.csv')"
      ],
      "metadata": {
        "id": "-S3jDfEtG9wG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
